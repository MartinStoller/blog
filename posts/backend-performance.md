# Backend-Performance in High-Throughput Systems - A Data Engineer's Perspective
Throughout my career in IT consulting, I have seen this pattern repeat itself over and over again:
Many high-load systems start out as small, inconspicuous prototypes.

In their early stages‚Äîwhen fundamental decisions about architecture, technologies, and design are made‚Äîthe scale at which 
these systems will eventually operate is often ignored or simply impossible to anticipate.

As a result, they are typically built using the default patterns of traditional backend development:

- a relational database with a highly normalized data model,
- an imperative, deterministic, object-oriented codebase,
- row-wise, case-by-case business logic,
- and a request‚Äìresponse architecture exposed via REST APIs and optimized for minimal latency.

These defaults exist for good reasons, and in many scenarios they work extremely well. However, some systems‚Äîespecially 
those facing massive data volumes or extreme request rates‚Äîare better served by a fundamentally different approach.

In this article, we will explore how to identify such systems and how backend developers can address performance challenges 
in high-throughput environments by deliberately breaking away from their default mental models. 
In particular, we will examine how core principles and technologies from the field of data engineering‚Äîsuch as batch 
processing, declarative computation, and denormalized data models‚Äîcan be applied to build backend systems with 
near-unbounded scalability.

## Meet Aunt Emma
Let us begin with an imaginary example, heavily inspired by real-world experiences from my own work.

Imagine our Aunt Emma running a small business: one store, two employees, and only a handful of different products‚Äînothing fancy. For many of her day-to-day business decisions, she relies on KPIs, sales forecasts, and price recommendations generated by quantitative business models.

Calculating all of this by hand‚Äîor with limited Excel skills‚Äîis tedious and error-prone. So, being the helpful niece or nephew that we are, we decide to automate the process by building a small web application.

You can imagine the UI looking roughly like this (thanks, ChatGPT!):
![img.png](../assets/aunt_emma/ui_aunt_emma.png)
Conceptually, the UI consists of two main components:

- An overview listing all sold products with some preview metrics (left side),
- A detailed view showing all available KPIs and data for a selected product (right side).

Our first step is to choose an appropriate architecture. We settle on what I would call a simple but reasonable approach:
![aunt_emma_architecture.drawio.svg](../assets/aunt_emma/aunt_emma_architecture.drawio.svg)

- How Aunt Emma stores her raw data does not matter for now. For the sake of simplicity, we also ignore how the data is ingested into our system. Whether it is streamed in real time via Kafka or delivered once per day by a carrier pigeon as a CSV file on a USB stick does not matter here. 
- The first concrete decision, then, is how to store the data. Naturally, we default to a normalized data model in a relational database. Given what we know at this point, this is a perfectly reasonable choice.
- On top of that, we introduce two backend services. The first one is the Pricing Engine, where all calculations and business logic live. The second is a generic Backend service, responsible for authentication, session and user management, and for orchestrating requests to the pricing engine. We already anticipate that these two services will have very different scaling characteristics, which is why we design them as separate components.
- Finally, we build a frontend that provides the user interface shown above.

\**Of course, it does not matter which programming language we use to implement these services. I happened to pick Java here, simply because it represents a traditional, best-practice-oriented style of software development particularly well in my mind.*

In summary, the request flow looks like this:
The user clicks on a product ‚ûú the backend requests all relevant KPIs from the pricing engine ‚ûú the pricing engine computes the results and returns them via the backend to the user.

At this point, everything feels straightforward‚Äîand it turns out to actually work perfectly fine! üéâ

There is just one small issue: some of the target data is expensive to compute, and it can take up to a minute before our beloved aunt sees all results in the UI. While this delay is acceptable for her‚Äîand still a major improvement compared to what she was working with before‚Äîwe quickly come up with an easy yet impactful optimization:

Why calculate all of this data on demand?
[LICENSE](../LICENSE)
Instead, we simply precompute all target data for all products ahead of time. The backend can then request everything directly from the database:
![emma_diagram_precompute.drawio.svg](../assets/aunt_emma/emma_diagram_precompute.drawio.svg)

From now on, every update in our database triggers the precomputation of all affected target data.

While this improvement is fairly simple‚Äîand you certainly don‚Äôt need to be a data engineer to come up with it‚Äîit already hints at a first potential pitfall.

If we stay too attached to our initial architectural design, we might instead try to ‚Äúfix‚Äù performance by reaching for familiar tools: more efficient algorithms, stronger hardware, smarter caching, better garbage collection, or additional tuning knobs. All of these can help, but they operate within the same fundamental request‚Äìresponse paradigm.

Being so accustomed to this model, it is easy to overlook that some systems behave very differently‚Äîand scale far better‚Äîonce this paradigm itself is questioned or even abandoned.

## Business is booming
Thanks to our well-functioning piece of software, Aunt Emma‚Äôs business has been growing rapidly over the years. She has opened multiple new shops, hired dozens of new employees, and even bootstrapped a small business intelligence team that now works with our software full time.

Naturally, this growth is also reflected in our data. What used to be a manageable amount of information has turned into a steadily growing data stream. Megabytes turned into gigabytes. While we once completed all precomputations within minutes, each update to the database now triggers hours of processing.

At this point, we clearly have a problem. We need to act ‚Äî but how?

Once again, there is no shortage of possible solutions. But instead of squeezing more performance out of our existing setup, let me propose another paradigm shift: **Batch Processing** (also known by the less glamorous name ETL).

If I had to give a textbook overview about batch processing vs traditional backend development it might look something like this:

| Aspect        | Batch / Analytical Processing (OLAP)               | Transactional Processing (OLTP)              |
|---------------|----------------------------------------------------|----------------------------------------------|
| Trigger       | Schedule                                           | Request                                      |
| Goal          | Throughput & idempotency                           | Low latency & determinism                    |
| Code Style    | Declarative / functional                           | Imperative / stateful                        |
| Logic         | Column-oriented & holistic                         | Row- / Object-oriented & single-case focused |
| Performance   | Optimized for total runtime & resource utilization | Optimized for response time                  |

While many backend developers are familiar with OLAP as a theoretical concept, in my experience very few are able to apply it effectively in practice. OLAP-style systems are not the norm in backend development ‚Äî but they are the norm in data engineering.
And if it wasn‚Äôt clear already, this is what this article is really about: building a better practical understanding of when, how, and why we should incorporate patterns from OLAP systems into backend architectures.

Coming back to our application, the obvious question now is: how do we actually apply these batch-processing principles? What needs to change ‚Äî and how will it help us overcome our growing performance problems?

The first point‚Äîchanging the trigger from request-driven to scheduled execution‚Äîwe have already covered in the previous chapter. But why should we change not only when our code runs, but also how it is written and structured?

The reason is that traditional object-oriented, imperative code is typically single-case focused. One object often corresponds to a single row in the database. Each row is deserialized into an object, business logic is applied case by case, and the resulting target data structures are then serialized again and written back to the database‚Äîone row at a time.

For problems like ours, this is a poor fit. In our case‚Äîand in fact in most precomputable scenarios‚Äîwe want to apply largely the same transformations and algorithms to all rows in a dataset. We want to compute the same KPIs for all products.

Declarative, column-oriented logic is a much better match for this type of workload. By describing what should be computed instead of how to compute it, we enable the execution engine to reason about the computation as a whole. This allows for global optimizations such as reordering operations, pushing down filters, batching work, and executing transformations in parallel.
In addition, column-oriented execution reduces serialization overhead, improves cache locality, and enables vectorized processing. The result is not only simpler code, but also significantly better resource utilization and throughput‚Äîespecially at scale.

So now we know when and why to turn to batch processing ‚Äî but what does this mean for our specific setup? In practice, it often means moving away from Java. While the Java ecosystem certainly offers batch-processing frameworks, they are not commonly used.
More typical approaches involve expressing large parts of the logic in SQL or using analytical libraries in Python such as Pandas, Polars, or DuckDB. SQL has the advantage of running directly inside the database engine, which can help avoid I/O-bound bottlenecks. However, once business logic reaches medium to high complexity, maintaining a large SQL-only codebase quickly becomes painful. 
(I once had the pleasure of working with a 30,000+ line, undocumented PL/SQL codebase. It‚Äôs not fun. Don‚Äôt do it.)
This is why I usually advocate for a Python-based approach. Modern analytical libraries provide an in-memory, column-oriented representation of data: so-called DataFrames. They allow us to express transformations in a declarative, SQL-like way, while still using Python‚Äôs language features to structure and modularize the code properly.
And before you ask: no, this does not mean that the system will be slower just because Python is involved. These libraries rarely execute Python bytecode for the actual data processing. Instead, Python primarily acts as an API layer for execution engines implemented in faster technologies such as C, the JVM, or Rust.

So, there we have it: our brand new BI-Tool 2.0:
![haeger_diagr_py.svg](../assets/aunt_emma/haeger_diagr_py.svg)

## Welcome to the World of Big Data
Once again, we have successfully addressed a critical performance bottleneck and scaled our system accordingly ü•≥
But at her weekly bingo night, Aunt Emma proudly tells her friends about the software that completely transformed her business. Among them, purely by coincidence, happens to be the CTO of Walmart‚Äîwho now wants us to build a similar solution for them.

When gigabytes turn into terabytes, things change fundamentally. At this scale, we have to go all in on our OLAP-style thinking. While discussing such a system in depth would go beyond the scope of this article‚Äîwhich is meant to provide backend developers with foundational concepts and guiding principles‚ÄîI will briefly outline the next set of challenges and ideas. I plan to dive into these topics in much more detail in future posts.
For now, here is a high-level overview of what becomes relevant at truly large scale:
1. **Data Modelling**: 

2. At large scale, we can apply a number of well-known storage strategies such as partitioning or bucketing to retrieve and process data more efficiently. The more fundamental paradigm shift, however, lies elsewhere: denormalization.

In distributed, scan-heavy analytical systems, joins quickly become one of the most expensive operations. To optimize read performance and simplify large-scale transformations, denormalized data models therefore become extremely useful.

That said, denormalization done right does not mean throwing all structure away and flattening everything indiscriminately. Instead, it requires a conscious decision about how much denormalization is needed and a deliberate, layered approach to data modeling.

A useful rule of thumb is this: every denormalized data structure should be derived from a normalized one. The normalized model serves as the single source of truth, while denormalized tables are optimized, derived representations tailored to specific access patterns and analytical use cases.

In practice, this often results in multiple modeling layers‚Äîranging from normalized core datasets to increasingly denormalized, query-optimized tables‚Äîeach serving a distinct purpose within the overall architecture.
2. **Persistence Technology**: File based storage(Iceberg, deltalake)... But querying the data with jdbc from the backend would become a challenge
3. **Distributed query engines**: ...

## Takeaway
Note this: If you think back to the initial setup with Aunt Emmas small shop: note how it was almost impossible to anticipate how much we will have to scale one day.
The prupose of this article is therefore not to tell you, that you were wrong all along with your normalized data models and object oriented imperative programming.
I consider all choices in the intial setup to be perfectly reasonable, given what we knew at the time.
Just know that as throughput increases more and more, you might want to deviate from your default structure more and more. It is not either OLTP or OLAP. It 
can be anything in between. Finding the right balance and neither optimizing prematurely nor too late is the real challenge. So please dont throw away your relational DB and refactor your 
entire persistence layer with Apache Iceberg, just because you had a slow response that one time.

