# Backend-Performance in High-Throughput Systems - A Data Engineer's Perspective
Throughout my career in IT consulting, I have seen this pattern repeat itself over and over again:
Many high-load systems start out as small, inconspicuous prototypes.

In their early stages‚Äîwhen fundamental decisions about architecture, technologies, and design are made‚Äîthe scale at which 
these systems will eventually operate is often ignored or simply impossible to anticipate.

As a result, they are typically built using the default patterns of classical backend development:

- a relational database with a highly normalized data model,
- an imperative, deterministic, object-oriented codebase,
- row-wise, case-by-case business logic,
- and a request‚Äìresponse architecture exposed via REST APIs and optimized for minimal latency.

These defaults exist for good reasons, and in many scenarios they work extremely well. However, some systems‚Äîespecially 
those facing massive data volumes or extreme request rates‚Äîare better served by a fundamentally different approach.

In this article, we will explore how to identify such systems and how backend developers can address performance challenges 
in high-throughput environments by deliberately breaking away from their default mental models. 
In particular, we will examine how core principles and technologies from the field of data engineering‚Äîsuch as batch 
processing, declarative computation, and denormalized data models‚Äîcan be applied to build backend systems with 
near-unbounded scalability.

## Meet Aunt Emma
Let us begin with an imaginary example, heavily inspired by real-world experiences from my own work.

Imagine our Aunt Emma running a small business: one store, two employees, and only a handful of different products‚Äînothing fancy. For many of her day-to-day business decisions, she relies on KPIs, sales forecasts, and price recommendations generated by quantitative business models.

Calculating all of this by hand‚Äîor with limited Excel skills‚Äîis tedious and error-prone. So, being the helpful niece or nephew that we are, we decide to automate the process by building a small web application.

You can imagine the UI looking roughly like this (thanks, ChatGPT!):
![img.png](../assets/aunt_emma/ui_aunt_emma.png)
Conceptually, the application consists of two main components:

- An overview listing all sold products with some preview metrics (left side),
- A detailed view showing all available KPIs and data for a selected product (right side).

Our first step is to choose an appropriate architecture. We settle on what I would call a simple but reasonable approach:
![aunt_emma_architecture.drawio.svg](../assets/aunt_emma/aunt_emma_architecture.drawio.svg)

- How Aunt Emma stores her raw data does not matter for now. For the sake of simplicity, we also ignore how the data is ingested into our system‚Äîwhether it is streamed in real time via Kafka or delivered once per day as a CSV file on a USB stick carried by a pigeon. 
- The first concrete decision, then, is how to store the data. Naturally, we default to a normalized data model in a relational database. Given what we know at this point, this is a perfectly reasonable choice.
- On top of that, we introduce two backend services. The first one is the Pricing Engine, where all calculations and business logic live. The second is a generic Backend service, responsible for authentication, session and user management, and for orchestrating requests to the pricing engine. We already anticipate that these two services will have very different scaling characteristics, which is why we design them as separate components.
- Finally, we build a frontend that provides the user interface shown above.

\**Of course, it does not matter which programming language we use to implement these services. I happened to pick Java here, simply because it represents a traditional, best-practice-oriented style of software development particularly well in my mind.*

In summary, the request flow looks like this:
The user clicks on a product ‚ûú the backend requests all relevant KPIs from the pricing engine ‚ûú the pricing engine computes the results and returns them via the backend to the user.

At this point, everything feels straightforward‚Äîand it turns out to actually work perfectly fine! üéâ

There is just one small issue: some of the target data is expensive to compute, and it can take up to a minute before our beloved aunt sees all results in the UI. While this delay is acceptable for her‚Äîand still a major improvement compared to what she was working with before‚Äîwe quickly come up with an easy yet impactful optimization:

Why calculate all of this data on demand?

Instead, we simply precompute all target data for all products ahead of time. The backend can then request everything directly from the database:
![emma_diagram_precompute.drawio.svg](../assets/aunt_emma/emma_diagram_precompute.drawio.svg)

From now on, every update in our database triggers the precomputation of all affected target data.

While this improvement is fairly simple‚Äîand you certainly don‚Äôt need to be a data engineer to come up with it‚Äîit already hints at a first potential pitfall.

If we stay too attached to our initial architectural design, we might instead try to ‚Äúfix‚Äù performance by reaching for familiar tools: more efficient algorithms, stronger hardware, smarter caching, better garbage collection, or additional tuning knobs. All of these can help, but they operate within the same fundamental request‚Äìresponse paradigm.

Being so accustomed to this model, it is easy to overlook that some systems behave very differently‚Äîand scale far better‚Äîonce this paradigm itself is questioned or even abandoned.

## Business is booming
Thanks to our well-working piece of software, Aunt Emma‚Äôs business has been growing rapidly over the years. She has opened multiple new shops, hired dozens of new employees, and even bootstrapped a small business intelligence team that now works with our software full time.

Naturally, this growth is also reflected in our data. What used to be a manageable amount of information has turned into a steadily growing data stream. While we once completed all precomputations within minutes, each update to the database now triggers hours of processing.

At this point, we clearly have a problem. We need to act ‚Äî but how?

Once again, there is no shortage of possible solutions. But instead of squeezing more performance out of our existing setup, let me propose another paradigm shift: **Batch Processing** (also known by the less glamorous name ETL).








Note this: If you think back to the initial setup with Aunt Emmas small shop: note how it was almost impossible to anticipate how much we will ahve to scale one day.
The prupose of this article is therefore not to tell you, that you were wrong all along with your normalized data models and object oriented imperative programming.
I consider all choices in the intial setup to be perfectly reasonable, given what we knew at the time.
Just know that as throughput increases more and more, you might want to deviate from your default structure more and more. It is not either OLTP or OLAP. It 
can be anything in between. Finding the right balance and neither optimizing prematurely nor too late is the real challenge. So please dont throw away your relational DB and refactor your 
entire persistence layer with Apache Iceberg, just because you had a slow response that one time.

