# Backend-Performance in High-Throughput Systems - A Data Engineer's Perspective
Throughout my career in IT consulting, I have seen this pattern repeat itself over and over again:
Many high-load systems start out as small, inconspicuous prototypes.

In their early stagesâ€”when fundamental decisions about architecture, technologies, and design are madeâ€”the scale at which 
these systems will eventually operate is often ignored or simply impossible to anticipate.

As a result, they are typically built using the default patterns of classical backend development:

- a relational database with a highly normalized data model,
- an imperative, deterministic, object-oriented codebase,
- row-wise, case-by-case business logic,
- and a requestâ€“response architecture exposed via REST APIs and optimized for minimal latency.

These defaults exist for good reasons, and in many scenarios they work extremely well. However, some systemsâ€”especially 
those facing massive data volumes or extreme request ratesâ€”are better served by a fundamentally different approach.

In this article, we will explore how to identify such systems and how backend developers can address performance challenges 
in high-throughput environments by deliberately breaking away from their default mental models. 
In particular, we will examine how core principles and technologies from the field of data engineeringâ€”such as batch 
processing, declarative computation, and denormalized data modelsâ€”can be applied to build backend systems with 
near-unbounded scalability.

## Meet Aunt Emma
Let us begin with an imaginary example, heavily inspired by real-world experiences from my own work.

Imagine our Aunt Emma running a small business: one store, two employees, and only a handful of different productsâ€”nothing fancy. For many of her day-to-day business decisions, she relies on KPIs, sales forecasts, and price recommendations generated by quantitative business models.

Calculating all of this by handâ€”or with limited Excel skillsâ€”is tedious and error-prone. So, being the helpful niece or nephew that we are, we decide to automate the process by building a small web application.

You can imagine the UI looking roughly like this (thanks, ChatGPT!):
![img.png](../assets/ui_aunt_emma.png)
Conceptually, the application consists of two main components:

- An overview listing all sold products with some preview metrics (left side),
- A detailed view showing all available KPIs and data for a selected product (right side).

Our first step is to choose an appropriate architecture. We settle on what I would call a simple but reasonable approach:
![aunt_emma_architecture.drawio.svg](../assets/aunt_emma_architecture.drawio.svg)

- How Aunt Emma stores her raw data does not matter for now. For the sake of simplicity, we also ignore how the data is ingested into our systemâ€”whether it is streamed in real time via Kafka or delivered once per day as a CSV file on a USB stick carried by a pigeon. 
- The first concrete decision, then, is how to store the data. Naturally, we default to a normalized data model in a relational database. Given what we know at this point, this is a perfectly reasonable choice.
- On top of that, we introduce two backend services. The first one is the Pricing Engine, where all calculations and business logic live. The second is a generic Backend service, responsible for authentication, session and user management, and for orchestrating requests to the pricing engine. We already anticipate that these two services will have very different scaling characteristics, which is why we design them as separate components.

- Finally, we build a frontend that provides the user interface shown above.

In summary, the request flow looks like this:
The user clicks on a product âžœ the backend requests all relevant KPIs from the pricing engine âžœ the pricing engine computes the results and returns them via the backend to the user.

At this point, everything feels straightforwardâ€”and, it turns out to actually work perfectly fine! ðŸŽ‰




Note this: How hard it can be sometimes to anticipate whether something will ever become a high-load system or forever 
remain a small nice prototype. 

