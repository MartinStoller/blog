{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Install depencencies:",
   "id": "c599ce512385d36"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-03T08:58:54.674067Z",
     "start_time": "2026-02-03T08:58:48.483046Z"
    }
   },
   "source": "!pip3 install pandas pyarrow numpy",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Collecting pandas\r\n",
      "  Downloading pandas-2.3.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (91 kB)\r\n",
      "Collecting pyarrow\r\n",
      "  Downloading pyarrow-21.0.0-cp39-cp39-macosx_12_0_arm64.whl.metadata (3.3 kB)\r\n",
      "Collecting numpy\r\n",
      "  Downloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl.metadata (60 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/martinstoller/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Collecting pytz>=2020.1 (from pandas)\r\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/martinstoller/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\r\n",
      "Downloading pandas-2.3.3-cp39-cp39-macosx_11_0_arm64.whl (10.8 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.8/10.8 MB\u001B[0m \u001B[31m30.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading pyarrow-21.0.0-cp39-cp39-macosx_12_0_arm64.whl (31.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m31.2/31.2 MB\u001B[0m \u001B[31m30.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl (5.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.3/5.3 MB\u001B[0m \u001B[31m29.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\r\n",
      "Installing collected packages: pytz, pyarrow, numpy, pandas\r\n",
      "\u001B[33m  WARNING: The scripts f2py and numpy-config are installed in '/Users/martinstoller/Library/Python/3.9/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\u001B[33m\r\n",
      "\u001B[0mSuccessfully installed numpy-2.0.2 pandas-2.3.3 pyarrow-21.0.0 pytz-2025.2\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define functions",
   "id": "a5b7707d18b5b1d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:07:14.196781Z",
     "start_time": "2026-02-03T10:07:14.183488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "OUTPUT_DIR = \"benchmark_data\"\n",
    "CSV_PATH = os.path.join(OUTPUT_DIR, \"data.csv\")\n",
    "PARQUET_PATH = os.path.join(OUTPUT_DIR, \"data.parquet\")\n",
    "\n",
    "TARGET_SIZE_GB = 1.0\n",
    "ROWS_PER_CHUNK = 5_000_000  # adjust if you hit memory limits\n",
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helper functions\n",
    "# -----------------------------\n",
    "def sizeof_gb(path):\n",
    "    return os.path.getsize(path) / (1024 ** 3)\n",
    "\n",
    "\n",
    "def timed(label, fn):\n",
    "    start = time.perf_counter()\n",
    "    result = fn()\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"{label:<35} {elapsed:8.2f} s\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Data generation\n",
    "# -----------------------------\n",
    "def generate_chunk(n):\n",
    "    return pd.DataFrame({\n",
    "        \"user_id\": np.random.randint(0, 1_000_000, size=n),\n",
    "        \"event_type\": np.random.randint(0, 50, size=n),\n",
    "        \"value\": np.random.randn(n) * 100,\n",
    "        \"timestamp\": np.random.randint(\n",
    "            1_600_000_000, 1_700_000_000, size=n\n",
    "        ),\n",
    "        \"category\": np.random.choice(\n",
    "            [\"A\", \"B\", \"C\", \"D\", \"E\"], size=n\n",
    "        ),\n",
    "    })\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Write CSV & Parquet\n",
    "# -----------------------------\n",
    "\n",
    "def write_csv():\n",
    "    if os.path.exists(CSV_PATH):\n",
    "        os.remove(CSV_PATH)\n",
    "\n",
    "    total_rows = 0\n",
    "    while not os.path.exists(CSV_PATH) or sizeof_gb(CSV_PATH) < TARGET_SIZE_GB:\n",
    "        df = generate_chunk(ROWS_PER_CHUNK)\n",
    "        df.to_csv(\n",
    "            CSV_PATH,\n",
    "            mode=\"a\",\n",
    "            index=False,\n",
    "            header=not os.path.exists(CSV_PATH),\n",
    "        )\n",
    "        total_rows += len(df)\n",
    "\n",
    "    return total_rows\n",
    "\n",
    "\n",
    "def write_parquet():\n",
    "    total_rows = 0\n",
    "    dfs = []\n",
    "\n",
    "    while sum(df.memory_usage(deep=True).sum() for df in dfs) < TARGET_SIZE_GB * (1024 ** 3):\n",
    "        dfs.append(generate_chunk(ROWS_PER_CHUNK))\n",
    "        total_rows += ROWS_PER_CHUNK\n",
    "\n",
    "    pd.concat(dfs, ignore_index=True).to_parquet(\n",
    "        PARQUET_PATH,\n",
    "        engine=\"pyarrow\",\n",
    "        compression=\"snappy\",\n",
    "    )\n",
    "    return total_rows"
   ],
   "id": "a82c6b3ffbbf573",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Writing synthetic data...\n",
    "Note, how parquet is not only fast but also much more compressed."
   ],
   "id": "c610fda5748624ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:08:37.768182Z",
     "start_time": "2026-02-03T10:08:04.706219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rows_csv = timed(\"Write CSV\", write_csv)\n",
    "rows_parquet = timed(\"Write Parquet\", write_parquet)\n",
    "\n",
    "print(f\"\\nCSV size:     {sizeof_gb(CSV_PATH):.2f} GB\")\n",
    "print(f\"Parquet size: {sizeof_gb(PARQUET_PATH):.2f} GB\")"
   ],
   "id": "a050fd3fdda12483",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write CSV                              29.37 s\n",
      "Write Parquet                           3.69 s\n",
      "\n",
      "CSV size:     1.16 GB\n",
      "Parquet size: 0.29 GB\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Read benchmarks",
   "id": "88344b4e621a057b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:10:16.796910Z",
     "start_time": "2026-02-03T10:10:10.718567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_csv = timed(\"Read CSV\", lambda: pd.read_csv(CSV_PATH))\n",
    "df_parquet = timed(\"Read Parquet\", lambda: pd.read_parquet(PARQUET_PATH))"
   ],
   "id": "4c2acd427bf5d5ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read CSV                                5.81 s\n",
      "Read Parquet                            0.26 s\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
